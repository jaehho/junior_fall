{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    %pip install numpy pandas matplotlib scikit-learn\n",
    "except ImportError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'SAheart.data'\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    os.remove(data_path)\n",
    "!wget https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.drop('row.names', axis=1)\n",
    "data['famhist'] = data['famhist'].map({'Absent': 0, 'Present': 1})\n",
    "data = data.sample(frac=1, random_state=2).reset_index(drop=True)\n",
    "\n",
    "display(data.describe().round(2))\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = data.drop('chd', axis=1)\n",
    "y = data['chd']\n",
    "features = X.columns\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot for the dataset\n",
    "sns.pairplot(data, hue='chd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train (80%), validation (10%), and test (10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "\n",
    "# Standardize the feature\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train, y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, learning_rate=0.01, C=0.01, num_steps=None, regularization=None, approach='vectorized'):\n",
    "        self.learning_rate = learning_rate  # Step size for updating weights\n",
    "        self.C = C  # Regularization strength (L1 or L2 penalty factor)\n",
    "        self.num_steps = num_steps if num_steps is not None else X.shape[0]\n",
    "        self.regularization = regularization  # 'l1', 'l2', or None\n",
    "        self.approach = approach  # 'vectorized' or 'stochastic'\n",
    "        self.weights = None\n",
    "        self.log_likelihood_values = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # S(z) = 1 / (1 + exp(-z))\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights))\n",
    "\n",
    "    def compute_log_likelihood(self, X, y):\n",
    "        # log_likelihood = sum(y * log(predictions) + (1 - y) * log(1 - predictions))\n",
    "        predictions = self.predict(X)\n",
    "        return np.sum(y * np.log(predictions + 1e-9) + (1 - y) * np.log(1 - predictions + 1e-9)) # 1e-9 is added to prevent log(0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize weights with zeros\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        \n",
    "        for step in range(self.num_steps):\n",
    "            X, y = shuffle(X, y, random_state=step)\n",
    "            \n",
    "            if self.approach == 'vectorized':\n",
    "                # Vectorized computation for all samples in the dataset\n",
    "                predictions = self.predict(X)\n",
    "                errors = y - predictions\n",
    "                gradient = np.dot(X.T, errors) / X.shape[0]\n",
    "                \n",
    "                # Update weights using the gradient and learning rate\n",
    "                self.weights += self.learning_rate * gradient\n",
    "            elif self.approach == 'stochastic':\n",
    "                # Stochastic gradient descent: iterate through each sample\n",
    "                for i in range(X.shape[0]):\n",
    "                    prediction = self.predict(X[i])\n",
    "                    error = y[i] - prediction\n",
    "                    gradient = X[i] * error\n",
    "                    \n",
    "                    # Update weights using the gradient and learning rate\n",
    "                    self.weights += self.learning_rate * gradient\n",
    "            \n",
    "            # Apply Regularization Penalty if regularization is enabled\n",
    "            if self.regularization == 'l1':\n",
    "                # L1 Regularization\n",
    "                self.weights = np.sign(self.weights) * np.maximum(0, np.abs(self.weights) - self.learning_rate * self.C)\n",
    "            elif self.regularization == 'l2':\n",
    "                # L2 Regularization\n",
    "                self.weights -= self.learning_rate * self.C * self.weights\n",
    "            \n",
    "            log_likelihood = self.compute_log_likelihood(X, y)\n",
    "            self.log_likelihood_values.append(log_likelihood)\n",
    "\n",
    "    def plot_log_likelihood(self):\n",
    "        # Plot the log likelihood values over iterations to visualize convergence\n",
    "        plt.plot(range(len(self.log_likelihood_values)), self.log_likelihood_values)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Log Likelihood')\n",
    "        plt.title('Log Likelihood Over Iterations')\n",
    "        plt.show()\n",
    "\n",
    "learning_rate = 0.01\n",
    "C = 0.01\n",
    "num_steps = 50\n",
    "approach = 'stochastic'\n",
    "\n",
    "# Train logistic regression using SGD without regularization\n",
    "model_no_reg = LogisticRegressionSGD(learning_rate=learning_rate, C=C, num_steps=num_steps, regularization=None, approach=approach)\n",
    "model_no_reg.fit(X_train, y_train.values)\n",
    "\n",
    "print(\"no_reg\", model_no_reg.log_likelihood_values[-1])\n",
    "model_no_reg.plot_log_likelihood()\n",
    "\n",
    "y_pred = model_no_reg.predict(X_test) >= 0.5\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Train logistic regression using SGDClassifier from scikit-learn\n",
    "sgd_logistic = SGDClassifier(loss='log_loss', max_iter=1000)\n",
    "sgd_logistic.fit(X_train, y_train)\n",
    "y_pred = sgd_logistic.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Forward Stepwise Feature Selection using Cross-Validation with LogisticRegressionSGD\n",
    "def forward_stepwise_selection(X_train, y_train, X_val, y_val, features, model):\n",
    "    best_score = 0\n",
    "    selected_features = []\n",
    "    available_features = list(features)\n",
    "\n",
    "    while available_features:\n",
    "        best_feature = None\n",
    "        for feature in available_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            X_train_subset = pd.DataFrame(X_train, columns=features)[current_features].values\n",
    "            X_val_subset = pd.DataFrame(X_val, columns=features)[current_features].values\n",
    "            \n",
    "            model.fit(X_train_subset, y_train.values)\n",
    "            predictions = model.predict(X_val_subset) >= 0.5\n",
    "            score = accuracy_score(y_val, predictions)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_feature = feature\n",
    "        \n",
    "        if best_feature is not None:\n",
    "            selected_features.append(best_feature)\n",
    "            available_features.remove(best_feature)\n",
    "        else:\n",
    "            break\n",
    "    return selected_features\n",
    "\n",
    "model = LogisticRegressionSGD(learning_rate=0.01, C=0.01, num_steps=100, regularization=None, approach='stochastic')\n",
    "selected_features = forward_stepwise_selection(X_train, y_train, X_val, y_val, features, model)\n",
    "\n",
    "print(\"Selected Features: \", selected_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
