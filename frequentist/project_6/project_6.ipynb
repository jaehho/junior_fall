{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 6: NMF\n",
    "\n",
    "Select a basic, explicit feedback dataset, many examples here:\n",
    "\n",
    "<https://gist.github.com/entaroadun/1653794>\n",
    "\n",
    "<https://github.com/caserec/Datasets-for-Recommender-Systems>\n",
    "\n",
    "<https://github.com/RUCAIBox/RecSysDatasets>\n",
    "\n",
    "And use the Surprise library:\n",
    "\n",
    "<http://surpriselib.com/>\n",
    "\n",
    "To implement a basic recommendation system. Many of those datasets are already loaded into the Surprise package to make this easy. You should tune and cross validate your system to select the best values for the # of latent dimensions, the regularization parameter, and any other hyperparameters.\n",
    "\n",
    "Stretch Goal #1 (3 points)\n",
    "\n",
    "Using an explicit feedback dataset, implement the NMF algorithm by hand, tune it via cross validation to select the # of latent dims and regularization parameter\n",
    "\n",
    "Stretch goal #2 (3 points)\n",
    "\n",
    "Using an implicit feedback dataset, some can be found here:\n",
    "\n",
    "<https://cseweb.ucsd.edu/~jmcauley/datasets.html>\n",
    "\n",
    "And implement the implicit feedback version of NMF. Evaluating performance will be harder however, so instead of doing a grid search and tuning, try to output the ratings explanations (i.e. use eqn 7 of the implicit feedback dataset).  Nobody has ever attempted this stretch goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NMF, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('196', '242', 3.0, '881250949'),\n",
       " ('186', '302', 3.0, '891717742'),\n",
       " ('22', '377', 1.0, '878887116'),\n",
       " ('244', '51', 2.0, '880606923'),\n",
       " ('166', '346', 1.0, '886397596')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MovieLens 100k dataset\n",
    "data = Dataset.load_builtin('ml-100k', prompt=False)\n",
    "data.raw_ratings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_factors': 10, 'n_epochs': 30, 'biased': False}\n",
      "Best RMSE: 1.0158539369859525\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "param_grid = {\n",
    "    'n_factors': [10, 20, 30], # number of latent factors\n",
    "    'n_epochs': [10, 20, 30], # number of epochs\n",
    "    'biased': [False] # use biases or not\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=5, n_jobs=-1)\n",
    "gs.fit(data)\n",
    "best_params = gs.best_params['rmse']\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best RMSE:\", gs.best_score['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0214\n",
      "Test RMSE: 1.0213946848067066\n"
     ]
    }
   ],
   "source": [
    "# Train the system\n",
    "test_size = 0.2\n",
    "trainset, testset = train_test_split(data, test_size=test_size)\n",
    "algo = NMF(**best_params)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "print(\"Test RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations for user 196: ['1639', '851', '814', '1175', '1449']\n"
     ]
    }
   ],
   "source": [
    "# Recommend items for a specific user\n",
    "user_id = '196'  # MovieLens user ID\n",
    "\n",
    "all_items = set(iid for (_, iid, _, _) in data.raw_ratings)\n",
    "rated_items = set(iid for (uid, iid, _, _) in data.raw_ratings if uid == str(user_id))\n",
    "unrated_items = list(all_items - rated_items)\n",
    "predictions = [(item, algo.predict(str(user_id), item).est) for item in unrated_items]\n",
    "recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(f\"Top recommendations for user {user_id}:\", [item for item, _ in recommendations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, NMF, accuracy\n",
    "from surprise.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "class RecommendationSystem:\n",
    "    def __init__(self, data: Dataset):\n",
    "        \"\"\"\n",
    "        Initialize the recommendation system with a Surprise dataset.\n",
    "        Args:\n",
    "            data (surprise.Dataset): Surprise dataset containing user-item ratings.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.algo = None\n",
    "        self.best_params = None\n",
    "\n",
    "    def tune_hyperparameters(self, param_grid: dict, cv: int = 5):\n",
    "        \"\"\"\n",
    "        Tune hyperparameters using GridSearchCV.\n",
    "        Args:\n",
    "            param_grid (dict): Dictionary of hyperparameter ranges for tuning.\n",
    "            cv (int): Number of cross-validation folds.\n",
    "        Returns:\n",
    "            float: Best RMSE score from hyperparameter tuning.\n",
    "        \"\"\"\n",
    "        gs = GridSearchCV(NMF, param_grid, measures=['rmse'], cv=cv, n_jobs=-1)\n",
    "        gs.fit(self.data)\n",
    "        self.best_params = gs.best_params['rmse']\n",
    "        return gs.best_score['rmse']\n",
    "\n",
    "    def train(self, test_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Train the recommendation system using the best hyperparameters.\n",
    "        Args:\n",
    "            test_size (float): Proportion of the dataset to use for testing.\n",
    "        Returns:\n",
    "            float: RMSE on the test set.\n",
    "        \"\"\"\n",
    "        trainset, testset = train_test_split(self.data, test_size=test_size)\n",
    "        self.algo = NMF(**self.best_params)\n",
    "        self.algo.fit(trainset)\n",
    "        predictions = self.algo.test(testset)\n",
    "        rmse = accuracy.rmse(predictions)\n",
    "        return rmse\n",
    "\n",
    "    def recommend(self, user_id, n=10):\n",
    "        \"\"\"\n",
    "        Recommend top-N items for a given user.\n",
    "        Args:\n",
    "            user_id (int): ID of the user.\n",
    "            n (int): Number of recommendations to return.\n",
    "        Returns:\n",
    "            list: List of recommended item IDs.\n",
    "        \"\"\"\n",
    "        all_items = set(iid for (_, iid, _, _) in self.data.raw_ratings)\n",
    "        rated_items = set(iid for (uid, iid, _, _) in self.data.raw_ratings if uid == str(user_id))\n",
    "        unrated_items = list(all_items - rated_items)\n",
    "\n",
    "        predictions = [(item, self.algo.predict(str(user_id), item).est) for item in unrated_items]\n",
    "        recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "        return [item for item, _ in recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 444447 entries, 0 to 444446\n",
      "Data columns (total 22 columns):\n",
      " #   Column                                 Non-Null Count   Dtype         \n",
      "---  ------                                 --------------   -----         \n",
      " 0   Facility ID                            444447 non-null  object        \n",
      " 1   Facility Name                          444447 non-null  object        \n",
      " 2   Address                                444447 non-null  object        \n",
      " 3   City/Town                              444447 non-null  object        \n",
      " 4   State                                  444447 non-null  object        \n",
      " 5   ZIP Code                               444447 non-null  int64         \n",
      " 6   County/Parish                          444447 non-null  object        \n",
      " 7   Telephone Number                       444447 non-null  object        \n",
      " 8   HCAHPS Measure ID                      444447 non-null  object        \n",
      " 9   HCAHPS Question                        444447 non-null  object        \n",
      " 10  HCAHPS Answer Description              444447 non-null  object        \n",
      " 11  Patient Survey Star Rating             444447 non-null  object        \n",
      " 12  Patient Survey Star Rating Footnote    17292 non-null   object        \n",
      " 13  HCAHPS Answer Percent                  444447 non-null  object        \n",
      " 14  HCAHPS Answer Percent Footnote         116496 non-null  object        \n",
      " 15  HCAHPS Linear Mean Value               444447 non-null  object        \n",
      " 16  Number of Completed Surveys            444447 non-null  object        \n",
      " 17  Number of Completed Surveys Footnote   150474 non-null  object        \n",
      " 18  Survey Response Rate Percent           444447 non-null  object        \n",
      " 19  Survey Response Rate Percent Footnote  150474 non-null  object        \n",
      " 20  Start Date                             444447 non-null  datetime64[ns]\n",
      " 21  End Date                               444447 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int64(1), object(19)\n",
      "memory usage: 74.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# # Load HCAHPS Hospital data from Excel file\n",
    "# df_hcahps = pd.read_excel('HCAHPS-Hospital.xlsx', usecols=['Facility ID', 'Patient Survey Star Rating'], nrows=100)\n",
    "df_hcahps = pd.read_excel('HCAHPS-Hospital.xlsx')\n",
    "df_hcahps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('196', '242', 3.0, '881250949'),\n",
       " ('186', '302', 3.0, '891717742'),\n",
       " ('22', '377', 1.0, '878887116'),\n",
       " ('244', '51', 2.0, '880606923'),\n",
       " ('166', '346', 1.0, '886397596')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MovieLens 100k dataset\n",
    "data = Dataset.load_builtin('ml-100k', prompt=False)\n",
    "data.raw_ratings[:5] # columns: user_id, item_id, rating, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_factors': 10, 'n_epochs': 30, 'biased': False}\n"
     ]
    }
   ],
   "source": [
    "# Tune the recommendation system\n",
    "recommender = RecommendationSystem(data)\n",
    "param_grid = {\n",
    "    'n_factors': [10, 20, 30], # number of latent factors\n",
    "    'n_epochs': [10, 20, 30], # number of epochs\n",
    "    'biased': [False] # use biases or not\n",
    "}\n",
    "recommender.tune_hyperparameters(param_grid)\n",
    "print(\"Best parameters:\", recommender.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0104\n",
      "Test RMSE: 1.0103570105967794\n"
     ]
    }
   ],
   "source": [
    "# Train the system and evaluate\n",
    "rmse = recommender.train()\n",
    "print(\"Test RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations for user 196: ['850', '1651', '868', '1450', '814']\n"
     ]
    }
   ],
   "source": [
    "# Recommend items for a specific user\n",
    "user_id = '196'  # MovieLens user ID as a string\n",
    "recommendations = recommender.recommend(user_id, n=5)\n",
    "print(f\"Top recommendations for user {user_id}:\", recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goal 1: NMF by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_alr(R:np.ndarray, n_components:int, max_iter:int=100, tol:float=1e-4, reg:float=0.0):\n",
    "    \"\"\"\n",
    "    Perform Non-Negative Matrix Factorization (NMF) using Alternating Linear Squares.\n",
    "    \n",
    "    Parameters:\n",
    "    - R: np.ndarray\n",
    "        The input matrix (m x n) to factorize.\n",
    "    - n_components: int\n",
    "        Number of latent dimensions (rank of the factorization).\n",
    "    - max_iter: int\n",
    "        Maximum number of iterations.\n",
    "    - tol: float\n",
    "        Convergence tolerance for reconstruction error.\n",
    "    - reg: float\n",
    "        Regularization parameter to avoid overfitting.\n",
    "    \n",
    "    Returns:\n",
    "    - W: np.ndarray\n",
    "        User feature matrix (m x n_components).\n",
    "    - H: np.ndarray\n",
    "        Item feature matrix (n_components x n).\n",
    "    - R_hat: np.ndarray\n",
    "        Reconstructed matrix (approximation of R).\n",
    "    \"\"\"\n",
    "    # Initialize W and H with random non-negative values\n",
    "    m, n = R.shape\n",
    "    W = np.random.rand(m, n_components)\n",
    "    H = np.random.rand(n_components, n)\n",
    "\n",
    "    # Alternating optimization\n",
    "    for iteration in range(max_iter):\n",
    "        # Fix H, solve for W\n",
    "        for i in range(m):\n",
    "            H_transpose = np.dot(H, H.T) + reg * np.eye(n_components)\n",
    "            W[i, :] = np.linalg.solve(H_transpose, np.dot(R[i, :], H.T))\n",
    "            W[i, :] = np.maximum(W[i, :], 0)  # Ensure non-negativity\n",
    "        \n",
    "        # Fix W, solve for H\n",
    "        for j in range(n):\n",
    "            W_transpose = np.dot(W.T, W) + reg * np.eye(n_components)\n",
    "            H[:, j] = np.linalg.solve(W_transpose, np.dot(W.T, R[:, j]))\n",
    "            H[:, j] = np.maximum(H[:, j], 0)  # Ensure non-negativity\n",
    "        \n",
    "        # Compute reconstruction error\n",
    "        R_hat = np.dot(W, H)\n",
    "        error = np.linalg.norm(R - R_hat, 'fro')\n",
    "\n",
    "        # Check for convergence\n",
    "        if error < tol:\n",
    "            break\n",
    "\n",
    "    return W, H, R_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix (R):\n",
      "[[5 3 0 1]\n",
      " [4 0 0 1]\n",
      " [1 1 0 5]\n",
      " [1 0 0 4]\n",
      " [0 1 5 4]]\n",
      "\n",
      "User Feature Matrix (W):\n",
      "[[0.         5.23556019]\n",
      " [0.         3.47585079]\n",
      " [1.13361791 1.31367621]\n",
      " [0.88795745 0.98363352]\n",
      " [1.69754708 0.        ]]\n",
      "\n",
      "Item Feature Matrix (H):\n",
      "[[0.         0.38861719 1.75622767 3.09010109]\n",
      " [1.0083973  0.38160565 0.         0.28233911]]\n",
      "\n",
      "Reconstructed Matrix (R_hat):\n",
      "[[5.27952477 1.99791937 0.         1.47820343]\n",
      " [3.50503856 1.32640431 0.         0.98136864]\n",
      " [1.32470754 0.94184967 1.99089113 3.87389611]\n",
      " [0.99189338 0.72043564 1.55945543 3.02159649]\n",
      " [0.         0.65969597 2.98127915 5.24559209]]\n"
     ]
    }
   ],
   "source": [
    "# Test input matrix\n",
    "R = np.array([[5, 3, 0, 1],\n",
    "              [4, 0, 0, 1],\n",
    "              [1, 1, 0, 5],\n",
    "              [1, 0, 0, 4],\n",
    "              [0, 1, 5, 4]])\n",
    "\n",
    "# Parameters\n",
    "n_components = 2\n",
    "max_iter = 500\n",
    "tol = 1e-4\n",
    "reg = 0.01\n",
    "\n",
    "# Get Feature matrices and reconstructed matrix\n",
    "W, H, R_hat = nmf_alr(R, n_components, max_iter, tol, reg)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Matrix (R):\")\n",
    "print(R)\n",
    "print(\"\\nUser Feature Matrix (W):\")\n",
    "print(W)\n",
    "print(\"\\nItem Feature Matrix (H):\")\n",
    "print(H)\n",
    "print(\"\\nReconstructed Matrix (R_hat):\")\n",
    "print(R_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.72341260e-01, 5.95639220e-01, 8.08451509e-02, ...,\n",
       "        2.12300976e-02, 2.12300976e-02, 2.12300976e-02],\n",
       "       [3.32431114e-01, 1.25727991e+00, 4.12760656e-02, ...,\n",
       "        1.50780448e-02, 1.50780448e-02, 1.50780448e-02],\n",
       "       [1.43603012e-01, 3.15933317e-01, 1.00313395e-01, ...,\n",
       "        1.86196042e-02, 1.86196042e-02, 1.86196042e-02],\n",
       "       ...,\n",
       "       [7.00483442e-01, 5.31157856e-01, 1.88152382e-02, ...,\n",
       "        2.26009384e-02, 2.26009384e-02, 2.26009384e-02],\n",
       "       [6.58500922e-01, 2.65107429e+00, 1.69687247e-04, ...,\n",
       "        2.62181393e-03, 2.62181393e-03, 2.62181393e-03],\n",
       "       [8.71384669e-02, 4.84739442e-01, 1.63240054e-02, ...,\n",
       "        1.52432328e-03, 1.52432328e-03, 1.52432328e-03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Surprise built-in MovieLens-100k dataset\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Convert the trainset to a user-item matrix\n",
    "def build_user_item_matrix(trainset):\n",
    "    R = np.zeros((trainset.n_users, trainset.n_items))\n",
    "    for (uid, iid, rating) in trainset.all_ratings():\n",
    "        R[int(uid), int(iid)] = rating\n",
    "    return R\n",
    "\n",
    "R = build_user_item_matrix(trainset)\n",
    "\n",
    "# Apply NMF to the user-item matrix\n",
    "n_components = 10  # Number of latent features\n",
    "W, H, R_hat = nmf_alr(R, n_components=n_components, max_iter=100, tol=1e-4, reg=0.01)\n",
    "R_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, R, R_hat, trainset, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommend top N items for a given user.\n",
    "\n",
    "    Parameters:\n",
    "    - user_id: str, raw user ID.\n",
    "    - R: np.ndarray, the original user-item matrix.\n",
    "    - R_hat: np.ndarray, the reconstructed user-item matrix (predictions).\n",
    "    - trainset: Trainset object, the Surprise trainset object.\n",
    "    - top_n: int, number of recommendations to return.\n",
    "\n",
    "    Returns:\n",
    "    - recommendations: list of tuples (item_id, predicted_rating).\n",
    "    \"\"\"\n",
    "    user_idx = trainset.to_inner_uid(user_id)  # Map raw user ID to internal ID\n",
    "    scores = R_hat[user_idx, :]  # Predicted scores for all items\n",
    "    # Get the list of items the user has already interacted with\n",
    "    known_items = [iid for (iid, _) in trainset.ur[user_idx]]  # Trainset internal item IDs\n",
    "\n",
    "    # Generate recommendations for items the user hasn't rated\n",
    "    recommendations = [\n",
    "        (trainset.to_raw_iid(iid), scores[iid])  # Map internal ID back to raw ID\n",
    "        for iid in range(len(scores)) if iid not in known_items\n",
    "    ]\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)  # Sort by predicted score\n",
    "    return recommendations[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top recommendations for user 196:\n",
      "Item: 275, Predicted Score: 1.31\n",
      "Item: 14, Predicted Score: 1.24\n",
      "Item: 83, Predicted Score: 1.12\n",
      "Item: 137, Predicted Score: 1.07\n",
      "Item: 283, Predicted Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Test the recommendation system\n",
    "user_id = '196'\n",
    "top_recommendations = recommend(user_id, R, R_hat, trainset, top_n=5)\n",
    "\n",
    "print(f\"Top recommendations for user {user_id}:\")\n",
    "for item, score in top_recommendations:\n",
    "    print(f\"Item: {item}, Predicted Score: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_nmf(R, latent_dims, reg_params, n_splits=5, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to tune the number of latent dimensions and regularization parameter.\n",
    "\n",
    "    Parameters:\n",
    "    - R: np.ndarray, the user-item matrix.\n",
    "    - latent_dims: list of int, list of latent dimensions to test.\n",
    "    - reg_params: list of float, list of regularization parameters to test.\n",
    "    - n_splits: int, number of folds for cross-validation.\n",
    "    - max_iter: int, maximum number of iterations for NMF.\n",
    "    - tol: float, tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "    - results: list of dict, each dict contains the hyperparameters and corresponding RMSE.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    results = []\n",
    "\n",
    "    for n_components in latent_dims:\n",
    "        for reg in reg_params:\n",
    "            fold_rmse = []\n",
    "            \n",
    "            for train_idx, test_idx in kf.split(R):\n",
    "                # Split data into train and test matrices\n",
    "                R_train = np.zeros_like(R)\n",
    "                R_test = np.zeros_like(R)\n",
    "                for idx in train_idx:\n",
    "                    R_train[idx] = R[idx]\n",
    "                for idx in test_idx:\n",
    "                    R_test[idx] = R[idx]\n",
    "\n",
    "                # Apply NMF on the training data\n",
    "                W, H, R_hat = nmf_alr(R_train, n_components, max_iter, tol, reg)\n",
    "\n",
    "                # Evaluate RMSE on the test set\n",
    "                mask = R_test > 0  # Only consider non-zero elements\n",
    "                rmse = np.sqrt(np.mean((R_test[mask] - R_hat[mask]) ** 2))\n",
    "                fold_rmse.append(rmse)\n",
    "\n",
    "            # Average RMSE across folds\n",
    "            avg_rmse = np.mean(fold_rmse)\n",
    "            results.append({'n_components': n_components, 'reg': reg, 'rmse': avg_rmse})\n",
    "            print(f\"n_components={n_components}, reg={reg}, RMSE={avg_rmse:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components=5, reg=0.0, RMSE=3.7095\n",
      "n_components=5, reg=0.01, RMSE=3.7095\n",
      "n_components=5, reg=0.1, RMSE=3.7095\n",
      "n_components=5, reg=0.5, RMSE=3.7095\n",
      "n_components=10, reg=0.0, RMSE=3.7095\n",
      "n_components=10, reg=0.01, RMSE=3.7095\n",
      "n_components=10, reg=0.1, RMSE=3.7095\n",
      "n_components=10, reg=0.5, RMSE=3.7095\n",
      "n_components=20, reg=0.0, RMSE=3.7095\n",
      "n_components=20, reg=0.01, RMSE=3.7095\n",
      "n_components=20, reg=0.1, RMSE=3.7095\n",
      "n_components=20, reg=0.5, RMSE=3.7095\n",
      "n_components=50, reg=0.0, RMSE=3.7095\n",
      "n_components=50, reg=0.01, RMSE=3.7095\n",
      "n_components=50, reg=0.1, RMSE=3.7095\n",
      "n_components=50, reg=0.5, RMSE=3.7095\n",
      "\n",
      "Best Hyperparameters:\n",
      "n_components: 5, reg: 0.0, RMSE: 3.7095\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid\n",
    "latent_dims = [5, 10, 20, 50]  # Number of latent features to test\n",
    "reg_params = [0.0, 0.01, 0.1, 0.5]  # Regularization parameters to test\n",
    "\n",
    "# Perform cross-validation\n",
    "results = cross_validate_nmf(R, latent_dims, reg_params, n_splits=5, max_iter=100, tol=1e-4)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best_result = min(results, key=lambda x: x['rmse'])\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(f\"n_components: {best_result['n_components']}, reg: {best_result['reg']}, RMSE: {best_result['rmse']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
